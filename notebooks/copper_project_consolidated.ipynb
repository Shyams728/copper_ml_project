{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Copper ML Project: Consolidated, Refactored Notebook\n",
        "\n",
        "This notebook merges the useful, non-duplicative parts of the existing notebooks into a single, well-commented pipeline. Each code block is preceded by a short explanation so a newcomer can follow the flow block-by-block."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports and global configuration\n",
        "We load the common data science stack (pandas/numpy/matplotlib/seaborn) and the ML tools used for preprocessing and modeling. Warnings are muted to keep the notebook output readable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load data\n",
        "Set a single source of truth for the dataset path so the notebook is easy to reuse. Replace the placeholder path with the real file location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_PATH = 'path/to/processed_Copper_Set_cleaned.xlsx'  # TODO: update with your local path\n",
        "\n",
        "df = pd.read_excel(DATA_PATH)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initial inspection\n",
        "Quickly review shape, column names, and types to understand the raw input. This aligns the EDA approach in the existing notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Shape:', df.shape)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Basic cleaning and normalization\n",
        "We normalize column names and drop columns with excessive missing values. This reflects the cleaning steps used in the industrial data cleaning notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standardize column names\n",
        "df.columns = [col.strip().lower().replace(' ', '_') for col in df.columns]\n",
        "\n",
        "# Drop columns with more than 50% missing values\n",
        "missing_threshold = 0.5\n",
        "df = df.dropna(thresh=df.shape[0] * (1 - missing_threshold), axis=1)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Missing data overview\n",
        "We quantify missing values so we can decide whether to impute, drop, or model around them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "missing_counts = df.isnull().sum().sort_values(ascending=False)\n",
        "missing_counts.head(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Outlier detection (IQR method)\n",
        "We use the IQR method to flag outliers in numerical columns, mirroring the EDA notebook's approach. This helps decide whether to clip, transform, or keep extreme values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numeric_cols = df.select_dtypes(include='number').columns\n",
        "\n",
        "def iqr_outlier_counts(dataframe, columns):\n",
        "    results = {}\n",
        "    for col in columns:\n",
        "        q1 = dataframe[col].quantile(0.25)\n",
        "        q3 = dataframe[col].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        lower = q1 - 1.5 * iqr\n",
        "        upper = q3 + 1.5 * iqr\n",
        "        outliers = dataframe[(dataframe[col] < lower) | (dataframe[col] > upper)]\n",
        "        results[col] = len(outliers)\n",
        "    return pd.Series(results).sort_values(ascending=False)\n",
        "\n",
        "iqr_outlier_counts(df, numeric_cols).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Log transformation for skewed numeric features\n",
        "We apply `np.log1p` to reduce skewness and handle zeros safely, following the EDA-ML notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "skewed_cols = df[numeric_cols].skew().sort_values(ascending=False)\n",
        "skewed_cols.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: apply log1p to the most skewed numeric columns\n",
        "top_skewed = skewed_cols.head(5).index\n",
        "for col in top_skewed:\n",
        "    df[f'log1p_{col}'] = np.log1p(df[col].clip(lower=0))\n",
        "\n",
        "df[[*top_skewed, *[f'log1p_{c}' for c in top_skewed]]].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Feature/target selection\n",
        "We prepare two targets: selling price (regression) and lead status (classification). Adjust the target column names to match your dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "REG_TARGET = 'selling_price'  # TODO: update if different\n",
        "CLS_TARGET = 'status'          # TODO: update if different\n",
        "\n",
        "feature_cols = [col for col in df.columns if col not in [REG_TARGET, CLS_TARGET]]\n",
        "X = df[feature_cols]\n",
        "y_reg = df[REG_TARGET]\n",
        "y_cls = df[CLS_TARGET]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Preprocessing pipeline\n",
        "Categorical features are one-hot encoded and numeric features are scaled. This keeps the modeling pipeline consistent across regression and classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "numeric_cols = X.select_dtypes(include='number').columns\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Regression modeling (selling price prediction)\n",
        "We train a RandomForestRegressor and evaluate with MAE, RMSE, and R\u00b2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X, y_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "regressor = RandomForestRegressor(random_state=42)\n",
        "reg_model = Pipeline(steps=[('preprocess', preprocessor), ('model', regressor)])\n",
        "\n",
        "reg_model.fit(X_train_reg, y_train_reg)\n",
        "reg_preds = reg_model.predict(X_test_reg)\n",
        "\n",
        "print('MAE:', mean_absolute_error(y_test_reg, reg_preds))\n",
        "print('RMSE:', mean_squared_error(y_test_reg, reg_preds, squared=False))\n",
        "print('R2:', r2_score(y_test_reg, reg_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Classification modeling (lead outcome prediction)\n",
        "We train a RandomForestClassifier and evaluate with a classification report and confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(\n",
        "    X, y_cls, test_size=0.2, random_state=42, stratify=y_cls\n",
        ")\n",
        "\n",
        "classifier = RandomForestClassifier(random_state=42)\n",
        "cls_model = Pipeline(steps=[('preprocess', preprocessor), ('model', classifier)])\n",
        "\n",
        "cls_model.fit(X_train_cls, y_train_cls)\n",
        "cls_preds = cls_model.predict(X_test_cls)\n",
        "\n",
        "print(classification_report(y_test_cls, cls_preds))\n",
        "confusion_matrix(y_test_cls, cls_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Next steps\n",
        "- Replace placeholder paths with the real data location.\n",
        "- Tune models (e.g., hyperparameters, cross-validation).\n",
        "- Add domain-specific feature engineering for price and lead classification.\n",
        "- Save the best models for downstream use."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}